import re
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_google_genai import GoogleGenerativeAIEmbeddings
from langchain_community.vectorstores import FAISS
from langchain_core.documents import Document
from langchain_upstage import UpstageEmbeddings
from rag_process.config import UPSTAGE_API_KEY

# TXT íŒŒì¼ ë¡œë“œ
with open("medicine_texts_ver3.txt", "r", encoding="utf-8") as f:
    text_data = f.read().strip()

# ğŸ”¹ 1. ì—°ì†ëœ ì¤„ë°”ê¿ˆì„ í†µì¼ëœ êµ¬ë¶„ìë¡œ ë³€ê²½ (ì œí’ˆëª… ë³€ê²½ ì‹œ ì‚¬ìš©)
text_data = re.sub(r"\n{3,}", "|||", text_data)  # 3ê°œ ì´ìƒì˜ ê°œí–‰ â†’ "|||"

# ğŸ”¹ 2. LangChain ì²­í‚¹ ì„¤ì • (êµ¬ë¶„ì ì„¤ì •)
text_splitter = RecursiveCharacterTextSplitter(
    separators=["|||", "\n\n", "\n"],  # ì œí’ˆ ë³€ê²½: "|||", ë¬¸ë‹¨: "\n\n", ë¬¸ì¥: ". "
    chunk_size=800,  # ì²­í¬ í¬ê¸°
    chunk_overlap=100,  # ë¬¸ë§¥ ìœ ì§€ìš© ì˜¤ë²„ë©
)

# ğŸ”¹ 3. ì²­í¬í™” ì‹¤í–‰
chunks = text_splitter.split_text(text_data)

# ğŸ”¹ 4. ê²°ê³¼ í™•ì¸
for i, chunk in enumerate(chunks[10:20]):  # ì²˜ìŒ 5ê°œ ì²­í¬ë§Œ í™•ì¸
    print(f"ì²­í¬ {i+1}:\n{chunk}\n{'-'*50}")



# API_KEY = os.getenv("GEMINI_API_KEY")

# embeddings_model = GoogleGenerativeAIEmbeddings(model='models/embedding-001', google_api_key=API_KEY) # ì„ë² ë”© ëª¨ë¸ ì„ íƒí•´ì•¼í•¨

embeddings_model = UpstageEmbeddings(model = "embedding-query", api_key = UPSTAGE_API_KEY)

documents = [Document(page_content=text) for text in chunks]

vectorstore = FAISS.from_documents(documents=documents, embedding=embeddings_model)

# FAISS ë¡œì»¬ ì €ì¥
vectorstore.save_local("medicine_faiss_upstage_ver3")

print("FAISS ë²¡í„° DB ì €ì¥ ì™„ë£Œ!")