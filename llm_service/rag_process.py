import os
from dotenv import load_dotenv
from langchain_openai import ChatOpenAI
from langchain_community.vectorstores import FAISS
from langchain_upstage import UpstageEmbeddings
from config import UPSTAGE_FAISS_DB_PATH, UPSTAGE_API_KEY
from config import OPEN_API_KEY, OPENAI_MODEL_NAME, MAX_OUTPUT_TOKENS, TEMPERATURE, TOP_P

#ì„ë² ë”© ëª¨ë¸ ë° ì €ì¥ëœ ë²¡í„° DB ë¡œë“œ
embeddings_model = UpstageEmbeddings(model="embedding-query", api_key=UPSTAGE_API_KEY)
vectorstore = FAISS.load_local(UPSTAGE_FAISS_DB_PATH, embeddings_model, allow_dangerous_deserialization=True)

#Generator ëª¨ë¸
openai_model = ChatOpenAI(
    model=OPENAI_MODEL_NAME,
    openai_api_key=OPEN_API_KEY,
    max_tokens=MAX_OUTPUT_TOKENS,
    temperature=TEMPERATURE,
    top_p=TOP_P
)

os.environ["OPENAI_API_KEY"] = OPEN_API_KEY

#í”„ë¡¬í”„íŠ¸ êµ¬ì„±

from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnablePassthrough
from langchain_core.messages import SystemMessage

# íŒŒì¼ì—ì„œ í”„ë¡¬í”„íŠ¸ ë‚´ìš©ì„ ì½ì–´ì˜¤ëŠ” í•¨ìˆ˜
def load_prompt(file_path):
    with open(file_path, "r", encoding="utf-8") as file:
        prompt = file.read().strip()
    return prompt

# 1. System í”„ë¡¬í”„íŠ¸ (ì—­í•  ì •ì˜)
prompt_template = load_prompt("rag_process/system_prompt.txt")


system_prompt_template = prompt_template.format(
    name="í™ì„±ì¤€",
    status="ì˜¤í›„ 12ì‹œ ë°œì—´ ì¦ìƒí•˜ì—¬ ì´ííœí‚¤ì¦ˆì•„ì´ì‹œëŸ½(ì´ë¶€í”„ë¡œíœ) 5ml ë³µìš©í•˜ì˜€ë‹¤."
)

formatted_system_prompt = SystemMessage(content=system_prompt_template)


# 2. User í”„ë¡¬í”„íŠ¸ (ì§ˆë¬¸ & context)
template = load_prompt("rag_process/prompt.txt")

prompt = ChatPromptTemplate.from_messages([formatted_system_prompt, template])

# Retriever ì„¤ì •ì •
retriever = vectorstore.as_retriever(
    search_type="similarity",
    search_kwargs={"k": 2}
)

chain = (
        {"context": retriever, "query": RunnablePassthrough()}
        | prompt
        | openai_model  
        | StrOutputParser()
    )

medicine = input("ì•½ ì´ë¦„ì„ ì…ë ¥í•˜ì„¸ìš”: ")          # ì˜ˆ: ì´ííœí‚¤ì¦ˆì•„ì´ì‹œëŸ½
information = input("ì–´ë–¤ ì •ë³´ë¥¼ ì›í•˜ì‹œë‚˜ìš”? (ì˜ˆ: ë¶€ì‘ìš©, ë³µìš©ë²•): ")  # ì˜ˆ: ë¶€ì‘ìš©

question = f"{medicine}ì˜ {information}ì„ ì•Œë ¤ì¤˜."

retrieved_docs = retriever.invoke(question)  

print("ê²€ìƒ‰ëœ ë¬¸ì„œ:")
for i, doc in enumerate(retrieved_docs):
    print(f"\nğŸ”¹ ë¬¸ì„œ {i+1}:\n{doc.page_content}")

response = chain.invoke(question)

print(response)
